{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINK TO GITHUB: [Ran+Shany Repo](https://github.com/RyanWri/Afeka_DL_course_labs/tree/main/src/task_2)\n",
    "\n",
    "**All our code is organized, you can find task2 in src/task_2 directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T07:12:42.666980Z",
     "start_time": "2024-07-28T07:12:41.144385Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nest_asyncio\n",
    "import json\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Allow nested use of asyncio.run()\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T07:12:42.675157Z",
     "start_time": "2024-07-28T07:12:42.667985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\Shany\\\\PycharmProjects\\\\Afeka_DL_course_labs\\\\src\\\\task_2'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Cell is for Import readibility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T07:12:42.680909Z",
     "start_time": "2024-07-28T07:12:42.676158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shany\\PycharmProjects\\Afeka_DL_course_labs\\src\\task_2\n"
     ]
    }
   ],
   "source": [
    "# Prepend the src directory to the PYTHONPATH\n",
    "sys.path.insert(0, os.path.abspath(os.getcwd()))\n",
    "\n",
    "# Verify that the path has been added\n",
    "print(sys.path[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Use the dataset from UCI Machine Learning Repository\n",
    "**We read the data and split it into 10 different chunks for faster reading, data files located in src/task_2/data directory**\n",
    "<br>*Asyncio is an asynchornous library for fast processing time*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Perform Exploratory Data Analysis (EDA) of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T07:12:54.164721Z",
     "start_time": "2024-07-28T07:12:42.681910Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.task_2.eda.eda import load_and_process_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T07:12:54.714252Z",
     "start_time": "2024-07-28T07:12:54.165727Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 19\u001B[0m\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m full_df\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Run the asynchronous processing\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m full_df \u001B[38;5;241m=\u001B[39m \u001B[43masyncio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_data_parallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Afeka_DL_course_labs\\.venv\\Lib\\site-packages\\nest_asyncio.py:30\u001B[0m, in \u001B[0;36m_patch_asyncio.<locals>.run\u001B[1;34m(main, debug)\u001B[0m\n\u001B[0;32m     28\u001B[0m task \u001B[38;5;241m=\u001B[39m asyncio\u001B[38;5;241m.\u001B[39mensure_future(main)\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_until_complete\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m task\u001B[38;5;241m.\u001B[39mdone():\n",
      "File \u001B[1;32m~\\PycharmProjects\\Afeka_DL_course_labs\\.venv\\Lib\\site-packages\\nest_asyncio.py:98\u001B[0m, in \u001B[0;36m_patch_loop.<locals>.run_until_complete\u001B[1;34m(self, future)\u001B[0m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m f\u001B[38;5;241m.\u001B[39mdone():\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m     97\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEvent loop stopped before Future completed.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 98\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py:203\u001B[0m, in \u001B[0;36mFuture.result\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__log_traceback \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 203\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\u001B[38;5;241m.\u001B[39mwith_traceback(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception_tb)\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:304\u001B[0m, in \u001B[0;36mTask.__step_run_and_handle_result\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m    300\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    301\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m exc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    302\u001B[0m         \u001B[38;5;66;03m# We use the `send` method directly, because coroutines\u001B[39;00m\n\u001B[0;32m    303\u001B[0m         \u001B[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001B[39;00m\n\u001B[1;32m--> 304\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mcoro\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    305\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    306\u001B[0m         result \u001B[38;5;241m=\u001B[39m coro\u001B[38;5;241m.\u001B[39mthrow(exc)\n",
      "Cell \u001B[1;32mIn[5], line 15\u001B[0m, in \u001B[0;36mprocess_data_parallel\u001B[1;34m()\u001B[0m\n\u001B[0;32m     12\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mgather(\u001B[38;5;241m*\u001B[39mtasks)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# Concatenate all chunks into a single DataFrame\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m full_df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresults\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m full_df\n",
      "File \u001B[1;32m~\\PycharmProjects\\Afeka_DL_course_labs\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001B[0m, in \u001B[0;36mconcat\u001B[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001B[0m\n\u001B[0;32m    379\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m copy \u001B[38;5;129;01mand\u001B[39;00m using_copy_on_write():\n\u001B[0;32m    380\u001B[0m     copy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m--> 382\u001B[0m op \u001B[38;5;241m=\u001B[39m \u001B[43m_Concatenator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    383\u001B[0m \u001B[43m    \u001B[49m\u001B[43mobjs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    384\u001B[0m \u001B[43m    \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    385\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    386\u001B[0m \u001B[43m    \u001B[49m\u001B[43mjoin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    387\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkeys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    388\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlevels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    389\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    390\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverify_integrity\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverify_integrity\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    391\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    392\u001B[0m \u001B[43m    \u001B[49m\u001B[43msort\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    393\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    395\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m op\u001B[38;5;241m.\u001B[39mget_result()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Afeka_DL_course_labs\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001B[0m, in \u001B[0;36m_Concatenator.__init__\u001B[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001B[0m\n\u001B[0;32m    442\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverify_integrity \u001B[38;5;241m=\u001B[39m verify_integrity\n\u001B[0;32m    443\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy \u001B[38;5;241m=\u001B[39m copy\n\u001B[1;32m--> 445\u001B[0m objs, keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_clean_keys_and_objs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobjs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    447\u001B[0m \u001B[38;5;66;03m# figure out what our result ndim is going to be\u001B[39;00m\n\u001B[0;32m    448\u001B[0m ndims \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_ndims(objs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Afeka_DL_course_labs\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001B[0m, in \u001B[0;36m_Concatenator._clean_keys_and_objs\u001B[1;34m(self, objs, keys)\u001B[0m\n\u001B[0;32m    504\u001B[0m     objs_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(objs)\n\u001B[0;32m    506\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(objs_list) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 507\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo objects to concatenate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    509\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m keys \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    510\u001B[0m     objs_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(com\u001B[38;5;241m.\u001B[39mnot_none(\u001B[38;5;241m*\u001B[39mobjs_list))\n",
      "\u001B[1;31mValueError\u001B[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "async def process_data_parallel():\n",
    "    rootdir = os.path.join(os.getcwd(), \"src\", \"task_2\", \"data\")\n",
    "\n",
    "    # Traverse Data Directory and get paths to all chunk files\n",
    "    file_names = []\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            file_names.append(os.path.join(subdir, file))\n",
    "    \n",
    "    # Load and process each chunk\n",
    "    tasks = [load_and_process_chunk(file) for file in file_names]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Concatenate all chunks into a single DataFrame\n",
    "    full_df = pd.concat(results)\n",
    "    return full_df\n",
    "\n",
    "# Run the asynchronous processing\n",
    "full_df = asyncio.run(process_data_parallel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot basic stats on data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T07:12:54.715267Z",
     "start_time": "2024-07-28T07:12:54.715267Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_dataframe_stats(df: pd.DataFrame):\n",
    "    # Display basic info about the DataFrame\n",
    "    print(df.info())\n",
    "    print(df.head())\n",
    "\n",
    "plot_dataframe_stats(full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Visualize Time Series Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_time_series_trends(df: pd.DataFrame):\n",
    "    # Plot Global_active_power over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df[\"Global_active_power\"], label=\"Global Active Power\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Global Active Power (kilowatts)\")\n",
    "    plt.title(\"Global Active Power over Time\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_time_series_trends(full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Check for Seasonality and Cyclical Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-28T07:12:54.718259Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_seasonality_and_cyclical_patterns(df: pd.DataFrame):\n",
    "    # Decompose the time series\n",
    "    decomposition = seasonal_decompose(\n",
    "        df[\"Global_active_power\"].dropna(), model=\"additive\", period=24 * 60\n",
    "    )\n",
    "\n",
    "    # Plot decomposition results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(411)\n",
    "    plt.plot(decomposition.observed, label=\"Observed\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.subplot(412)\n",
    "    plt.plot(decomposition.trend, label=\"Trend\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.subplot(413)\n",
    "    plt.plot(decomposition.seasonal, label=\"Seasonal\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.subplot(414)\n",
    "    plt.plot(decomposition.resid, label=\"Residual\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "check_seasonality_and_cyclical_patterns(full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Analyze Distribution of Power Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_distribution_of_power_consumption(df: pd.DataFrame):\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    df[\"Global_active_power\"].hist(bins=50)\n",
    "    plt.xlabel(\"Global Active Power (kilowatts)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Global Active Power\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot boxplot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    df.boxplot(column=\"Global_active_power\")\n",
    "    plt.ylabel(\"Global Active Power (kilowatts)\")\n",
    "    plt.title(\"Boxplot of Global Active Power\")\n",
    "    plt.show()\n",
    "\n",
    "analyze_distribution_of_power_consumption(full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implement a linear regression model to predict power consumption for the last three time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-28T07:12:54.719255Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.task_2.models.linear_regression import split_data_append_lagged_features, run_linear_regression\n",
    "from src.task_2.evaluation.model_evaluation import run_model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-28T07:12:54.720252Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.task_2.evaluation.model_evaluation import run_model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-28T07:12:54.721251Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_data_append_lagged_features(full_df)\n",
    "linear_reg = run_linear_regression(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_lr = linear_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "lr_results = run_model_evaluation(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate the linear regression model using appropriate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-28T07:12:54.722246Z"
    }
   },
   "outputs": [],
   "source": [
    "print(json.dumps(lr_results, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Implement a Recurrent Neural Network (RNN) for power consumption prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-28T07:12:54.723247Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.task_2.models.rnn import run_rnn_model_e2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-28T07:12:54.724249Z"
    }
   },
   "outputs": [],
   "source": [
    "run_rnn_model_e2e(full_df, epochs=6, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Implement Long Short-Term Memory (LSTM) for power consumption prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-28T07:12:54.725246Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.task_2.models.long_short_term_memory import run_lstm_model_e2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-28T07:12:54.726243Z"
    }
   },
   "outputs": [],
   "source": [
    "run_lstm_model_e2e(full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Implement an LSTM model with an Attention layer for power consumption prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-28T07:12:54.727240Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.task_2.models.lstm_with_attention import run_lstm_with_attention_e2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-28T07:12:54.728237Z"
    }
   },
   "outputs": [],
   "source": [
    "run_lstm_with_attention_e2e(full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Data augmentation experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-28T07:12:54.729234Z"
    }
   },
   "outputs": [],
   "source": [
    "from task_2.data_modification.augmentation import add_noise, scale_data, shift_data, window_slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = np.concatenate([X, add_noise(X), scale_data(X), shift_data(X), window_slicing(X)], axis=0)\n",
    "y_combined = np.concatenate([y, y, y, y, y], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_augment, X_test_augment, y_train_augment, y_test_augment = rnn_split(X_combined, y_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Data reduction experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.task_2.data_modification.augmentation import reduce_data_randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-28T07:12:54.735218Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply reduction\n",
    "X_reduced, y_reduced = reduce_data_randomly(X, y, reduction_factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-28T07:12:54.736216Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Data resolution experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-28T07:12:54.737213Z"
    }
   },
   "outputs": [],
   "source": [
    "from data_modification.resolution import resample_data_from_1min_to_2min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_df = resample_data_from_1min_to_2min(full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Conclusion and insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
